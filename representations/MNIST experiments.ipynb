{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.utils import check_random_state\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "from theano import shared\n",
    "from time import time\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Creating train-test split...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "def load_data(dtype=np.float32, order='C'):\n",
    "    \"\"\"Load the data, then cache and memmap the train/test split\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    data = fetch_mldata('MNIST original')\n",
    "    X = check_array(data['data'], dtype=dtype, order=order)\n",
    "    y = data[\"target\"]\n",
    "    # Scale the features to [0, 1]\n",
    "    X = X / 255\n",
    "    ## Create train-test split (as [Joachims, 2006])\n",
    "    print(\"Creating train-test split...\")\n",
    "    n_train = 60000\n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = load_data()\n",
    "\n",
    "# Create a small training set for faster experiments\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_dev, y_dev, train_size=int(1e4), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various neural networks utilities\n",
    "\n",
    "Mostly taken from Alec Radford and Kyle Kastner's gists:\n",
    "\n",
    "- https://gist.github.com/Newmu/a56d5446416f5ad2bbac\n",
    "- https://gist.github.com/kastnerkyle/f3f67424adda343fef40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sharedX(X, dtype=theano.config.floatX, name=None):\n",
    "    return theano.shared(np.asarray(X, dtype=dtype), name=name)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return tt.nnet.softmax(x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return (x + abs(x)) / 2.0\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return tt.tanh(x)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return tt.nnet.sigmoid(x)\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def iter_data(*data, **kwargs):\n",
    "    batch_size = kwargs.get('batch_size', 128)\n",
    "    batches, remainder = divmod(len(data[0]), batch_size)\n",
    "    if remainder != 0:\n",
    "        batches += 1\n",
    "    for b in range(batches):\n",
    "        start = b * batch_size\n",
    "        end = (b + 1) * batch_size\n",
    "        if len(data) == 1:\n",
    "            yield data[0][start:end]\n",
    "        else:\n",
    "            yield tuple([d[start:end] for d in data])\n",
    "            \n",
    "\n",
    "class Adam(object):\n",
    "    \"\"\"no bias init correction and no b1 decay\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.0001, grad_momentum=0.1,\n",
    "                 sq_grad_momentum=0.001, eps=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_momentum = grad_momentum\n",
    "        self.sq_grad_momentum = sq_grad_momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def make_updates(self, params, cost):\n",
    "        b1 = self.grad_momentum\n",
    "        b2 = self.sq_grad_momentum\n",
    "        updates = []\n",
    "        grads = tt.grad(cost, params)\n",
    "        for p, g in zip(params, grads):\n",
    "            m = shared(p.get_value() * 0.)\n",
    "            v = shared(p.get_value() * 0.)\n",
    "            m_t = (b1 * g) + ((1. - b1) * m)\n",
    "            v_t = (b2 * tt.sqr(g)) + ((1. - b2) * v)\n",
    "            g_t = m_t / (tt.sqrt(v_t) + self.eps)\n",
    "            p_t = p - (self.learning_rate * g_t)\n",
    "            updates.append((m, m_t))\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((p, p_t))\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingMonitor(object):\n",
    "    def __init__(self, X_val=None, y_val=None, patience=3, data_log_filename=None):\n",
    "        if X_val is not None:\n",
    "            X_val, y_val = check_X_y(X_val, y_val, dtype=theano.config.floatX)\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.data_log_filename = data_log_filename\n",
    "        \n",
    "        self.best_cost = np.inf\n",
    "        self.patience = patience\n",
    "        self.patience_reset = patience\n",
    "        \n",
    "    def monitor(self, model, context):\n",
    "        epoch = context.get('epoch')\n",
    "        X_train = context.get('X')\n",
    "        Y_train = context.get('Y')\n",
    "        \n",
    "        X_val = self.X_val\n",
    "        Y_val = getattr(self, 'Y_val', None)\n",
    "        if Y_val is None and self.y_val is not None:\n",
    "            self.Y_val = Y_val = model.label_binarizer_.transform(self.y_val)\n",
    "        \n",
    "        training_cost = model._compute_cost(X_train, Y_train)\n",
    "        if X_val is not None:\n",
    "            validation_cost = model._compute_cost(X_val, Y_val)\n",
    "            current_cost = validation_cost\n",
    "                \n",
    "        else:\n",
    "            validation_cost = np.nan\n",
    "            current_cost = training_cost\n",
    "\n",
    "        print(\"Epoch #%03d: training cost: %0.5f, validation cost: %0.5f\" % (\n",
    "            epoch, training_cost, validation_cost))\n",
    "\n",
    "        # Patience-based stopping condition\n",
    "        if current_cost < self.best_cost:\n",
    "            self.patience = self.patience_reset\n",
    "            self.best_cost = current_cost\n",
    "        else:\n",
    "            self.patience -= 1\n",
    "        return self.patience > 0\n",
    "\n",
    "\n",
    "class MLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, hidden=(100, 100), activation='relu', init_gain='auto',\n",
    "                 batch_size=128, optimizer=None, n_epochs=1000,\n",
    "                 warm_start=False, random_state=None):\n",
    "        self.hidden = hidden\n",
    "        self.activation = activation\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.n_epochs = n_epochs\n",
    "        self.init_gain = init_gain\n",
    "        self.warm_start = warm_start\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _init_parameters(self, n_features, n_outputs):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        input_dims = (n_features,) + self.hidden\n",
    "        output_dims = self.hidden + (n_outputs,)\n",
    "\n",
    "        if self.init_gain == 'auto':\n",
    "            g = np.sqrt(2) if self.activation == 'relu' else 1.\n",
    "        else:\n",
    "            g = self.init_gain\n",
    "        \n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        for l, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
    "            std = 2 * g / (in_dim + out_dim)\n",
    "            W = sharedX(rng.normal(size=(in_dim, out_dim), scale=std))\n",
    "            self.weights_.append(W)\n",
    "            b = sharedX(np.zeros(out_dim))\n",
    "            self.biases_.append(b)\n",
    "        \n",
    "    def _make_functions(self):\n",
    "        x = tt.matrix()\n",
    "        y = tt.matrix()\n",
    "\n",
    "        # Define the computation graph of the model\n",
    "        if self.activation == 'relu':\n",
    "            sigma = relu\n",
    "        elif self.activation == 'tanh':\n",
    "            sigma = tanh\n",
    "        elif self.activation == 'linear':\n",
    "            sigma = linear\n",
    "        else:\n",
    "            raise ValueError('Unsupported activation: %s' % self.activation)\n",
    "        \n",
    "        activations = [sigma] * (len(self.weights_) - 1) + [softmax]\n",
    "        tmp = x\n",
    "        for w, b, s in zip(self.weights_, self.biases_, activations):\n",
    "            tmp = s(tt.dot(tmp, w) + b)\n",
    "        \n",
    "        output = tmp\n",
    "        cost = tt.nnet.binary_crossentropy(output, y).mean()\n",
    "        \n",
    "        # Use the optimizer to compute the parameter updates based\n",
    "        # on the gradient of the cost function\n",
    "        opt = self.optimizer\n",
    "        if opt is None:\n",
    "            opt = Adam()\n",
    "        parameters = []\n",
    "        parameters += self.weights_\n",
    "        parameters += self.biases_\n",
    "        fit_updates = opt.make_updates(parameters, cost)\n",
    "        \n",
    "        # Compile the functions them-selves\n",
    "        f = theano.function\n",
    "        self._fit = f([x, y], cost, updates=fit_updates, name='_fit')\n",
    "        self._compute_cost = f([x, y], cost, name=\"_compute_cost\")\n",
    "        self._forward = f([x], output, name=\"_forward\")\n",
    "\n",
    "    def fit(self, X, y, monitor=None):\n",
    "        X, y = check_X_y(X, y, dtype=theano.config.floatX)\n",
    "        self.label_binarizer_ = lb = LabelBinarizer()\n",
    "        Y = lb.fit_transform(y).astype(theano.config.floatX)\n",
    "        n_samples, n_features = X.shape\n",
    "        _, n_outputs = Y.shape\n",
    "        if not self.warm_start or not hasattr(self, 'weights_') :\n",
    "            self._init_parameters(n_features, n_outputs)\n",
    "\n",
    "        self._make_functions()\n",
    "        self.training_costs_ = [self._compute_cost(X, Y)]\n",
    "        for epoch in range(self.n_epochs):\n",
    "            if monitor is not None and not monitor.monitor(self, locals()):\n",
    "                break\n",
    "            for X_batch, Y_batch in iter_data(X, Y):\n",
    "                cost = self._fit(X_batch, Y_batch)\n",
    "                self.training_costs_.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = check_array(X, dtype=theano.config.floatX)\n",
    "        return self._forward(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        indices = self.predict_proba(X).argmax(axis=1)\n",
    "        return self.label_binarizer_.classes_[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #000: training cost: 0.41119, validation cost: 0.41400\n",
      "Epoch #001: training cost: 0.06485, validation cost: 0.07053\n",
      "Epoch #002: training cost: 0.04739, validation cost: 0.05612\n",
      "Epoch #003: training cost: 0.03956, validation cost: 0.05027\n",
      "Epoch #004: training cost: 0.03457, validation cost: 0.04674\n",
      "Epoch #005: training cost: 0.03094, validation cost: 0.04433\n",
      "Epoch #006: training cost: 0.02814, validation cost: 0.04256\n",
      "Epoch #007: training cost: 0.02586, validation cost: 0.04111\n",
      "Epoch #008: training cost: 0.02397, validation cost: 0.04000\n",
      "Epoch #009: training cost: 0.02230, validation cost: 0.03904\n",
      "Epoch #010: training cost: 0.02084, validation cost: 0.03819\n",
      "Epoch #011: training cost: 0.01954, validation cost: 0.03747\n",
      "Epoch #012: training cost: 0.01838, validation cost: 0.03685\n",
      "Epoch #013: training cost: 0.01732, validation cost: 0.03627\n",
      "Epoch #014: training cost: 0.01635, validation cost: 0.03578\n",
      "Epoch #015: training cost: 0.01546, validation cost: 0.03530\n",
      "Epoch #016: training cost: 0.01465, validation cost: 0.03490\n",
      "Epoch #017: training cost: 0.01389, validation cost: 0.03451\n",
      "Epoch #018: training cost: 0.01317, validation cost: 0.03416\n",
      "Epoch #019: training cost: 0.01250, validation cost: 0.03383\n",
      "Epoch #020: training cost: 0.01187, validation cost: 0.03354\n",
      "Epoch #021: training cost: 0.01129, validation cost: 0.03328\n",
      "Epoch #022: training cost: 0.01072, validation cost: 0.03302\n",
      "Epoch #023: training cost: 0.01020, validation cost: 0.03282\n",
      "Epoch #024: training cost: 0.00970, validation cost: 0.03262\n",
      "Epoch #025: training cost: 0.00925, validation cost: 0.03245\n",
      "Epoch #026: training cost: 0.00880, validation cost: 0.03230\n",
      "Epoch #027: training cost: 0.00840, validation cost: 0.03216\n",
      "Epoch #028: training cost: 0.00800, validation cost: 0.03204\n",
      "Epoch #029: training cost: 0.00762, validation cost: 0.03192\n",
      "Epoch #030: training cost: 0.00726, validation cost: 0.03182\n",
      "Epoch #031: training cost: 0.00692, validation cost: 0.03171\n",
      "Epoch #032: training cost: 0.00659, validation cost: 0.03164\n",
      "Epoch #033: training cost: 0.00627, validation cost: 0.03158\n",
      "Epoch #034: training cost: 0.00599, validation cost: 0.03155\n",
      "Epoch #035: training cost: 0.00570, validation cost: 0.03148\n",
      "Epoch #036: training cost: 0.00542, validation cost: 0.03143\n",
      "Epoch #037: training cost: 0.00516, validation cost: 0.03140\n",
      "Epoch #038: training cost: 0.00491, validation cost: 0.03132\n",
      "Epoch #039: training cost: 0.00468, validation cost: 0.03134\n",
      "Epoch #040: training cost: 0.00445, validation cost: 0.03127\n",
      "Epoch #041: training cost: 0.00424, validation cost: 0.03129\n",
      "Epoch #042: training cost: 0.00402, validation cost: 0.03128\n",
      "Epoch #043: training cost: 0.00383, validation cost: 0.03128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', batch_size=128, hidden=(100, 100),\n",
       "       init_gain=20.0, n_epochs=1000, optimizer=None, random_state=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden=(100, 100), batch_size=128, init_gain=20., random_state=0)\n",
    "mlp.fit(X_train, y_train, monitor=EarlyStoppingMonitor(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95269999999999999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
