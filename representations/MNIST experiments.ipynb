{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import shuffle\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "from theano import shared\n",
    "from time import time\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams \n",
    "floatX =theano.config.floatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Creating train-test split...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "def load_data(dtype=np.float32, order='C'):\n",
    "    \"\"\"Load the data, then cache and memmap the train/test split\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    data = fetch_mldata('MNIST original')\n",
    "    X = check_array(data['data'], dtype=dtype, order=order)\n",
    "    y = data[\"target\"]\n",
    "    # Scale the features to [0, 1]\n",
    "    X = X / 255\n",
    "    ## Create train-test split (as [Joachims, 2006])\n",
    "    print(\"Creating train-test split...\")\n",
    "    n_train = 60000\n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = load_data()\n",
    "\n",
    "# Create a small training set for faster experiments\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_dev, y_dev, test_size=int(1e3), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various neural networks utilities\n",
    "\n",
    "Mostly taken from Alec Radford and Kyle Kastner's gists:\n",
    "\n",
    "- https://gist.github.com/Newmu/a56d5446416f5ad2bbac\n",
    "- https://gist.github.com/kastnerkyle/f3f67424adda343fef40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sharedX(X, dtype=floatX, name=None):\n",
    "    return theano.shared(np.asarray(X, dtype=dtype), name=name)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return tt.nnet.softmax(x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return (x + abs(x)) / 2.0\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return tt.tanh(x)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return tt.nnet.sigmoid(x)\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def iter_data(*data, **kwargs):\n",
    "    batch_size = kwargs.get('batch_size', 128)\n",
    "    batches, remainder = divmod(len(data[0]), batch_size)\n",
    "    if remainder != 0:\n",
    "        batches += 1\n",
    "    for b in range(batches):\n",
    "        start = b * batch_size\n",
    "        end = (b + 1) * batch_size\n",
    "        if len(data) == 1:\n",
    "            yield data[0][start:end]\n",
    "        else:\n",
    "            yield tuple([d[start:end] for d in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    \"\"\"no bias init correction and no b1 decay\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, grad_momentum=0.1,\n",
    "                 sq_grad_momentum=0.001, eps=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_momentum = grad_momentum\n",
    "        self.sq_grad_momentum = sq_grad_momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def make_updates(self, params, cost):\n",
    "        b1 = self.grad_momentum\n",
    "        b2 = self.sq_grad_momentum\n",
    "        updates = []\n",
    "        grads = tt.grad(cost, params)\n",
    "        for p, g in zip(params, grads):\n",
    "            m = shared(p.get_value() * 0.)\n",
    "            v = shared(p.get_value() * 0.)\n",
    "            m_t = (b1 * g) + ((1. - b1) * m)\n",
    "            v_t = (b2 * tt.sqr(g)) + ((1. - b2) * v)\n",
    "            g_t = m_t / (tt.sqrt(v_t) + self.eps)\n",
    "            p_t = p - self.learning_rate * g_t\n",
    "            updates.append((m, m_t))\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((p, p_t))\n",
    "        return updates\n",
    "    \n",
    "    \n",
    "class MomentumSGD(object):\n",
    "    \n",
    "    monitor = None\n",
    "    \n",
    "    def __init__(self, learning_rate_init=0.1, max_adaptation_count=3,\n",
    "                 learning_rate_scale=0.2, momentum=0.9):\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_adaptation_count = max_adaptation_count\n",
    "        self.learning_rate_scale = learning_rate_scale\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def on_stalled_convergence(self):\n",
    "        if self.adaptation_count > self.max_adaptation_count:\n",
    "            # Optimizer thinks model has converged\n",
    "            return False\n",
    "        \n",
    "        # Rescale the learning rate\n",
    "        old_lr = self.learning_rate.get_value()\n",
    "        new_lr = old_lr * self.learning_rate_scale\n",
    "        self.learning_rate.set_value(new_lr)\n",
    "        self.adaptation_count += 1\n",
    "        print(\"Updated learning rate from %f to %f\" % (old_lr, new_lr))\n",
    "\n",
    "    def make_updates(self, params, cost):\n",
    "        self.adaptation_count = 0\n",
    "        self.learning_rate = sharedX(self.learning_rate_init)\n",
    "\n",
    "        updates = []\n",
    "        grads = tt.grad(cost, params)\n",
    "        for p, g in zip(params, grads):\n",
    "            if self.momentum > 0:\n",
    "                v = shared(p.get_value() * 0.)\n",
    "                v_t = self.momentum * v - self.learning_rate * g\n",
    "                p_t = p + v_t\n",
    "                updates.append((v, v_t))\n",
    "            else:\n",
    "                # traditional SGD\n",
    "                p_t = p - self.learning_rate * g\n",
    "            updates.append((p, p_t))\n",
    "        return updates\n",
    "\n",
    "    \n",
    "class NesterovMomentumSGD(MomentumSGD):\n",
    "\n",
    "    def make_updates(self, params, cost):\n",
    "        self.adaptation_count = 0\n",
    "        self.learning_rate = sharedX(self.learning_rate_init)\n",
    "\n",
    "        updates = []\n",
    "        grads = tt.grad(cost, params)\n",
    "        for p, g in zip(params, grads):\n",
    "            g = tt.clip(g, -.001, .001)\n",
    "            v = shared(p.get_value() * 0.)\n",
    "            v_t = self.momentum * v - self.learning_rate * g\n",
    "            p_t = p + self.momentum * v_t - self.learning_rate * g\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((p, p_t))\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingMonitor(object):\n",
    "    def __init__(self, X_train, y_train, X_val=None, y_val=None, patience=3,\n",
    "                 data_log_filename=None, subsample=int(5e3), random_state=None):\n",
    "\n",
    "        X_train, y_train = check_X_y(X_train, y_train, dtype=floatX)\n",
    "        self.label_binarizer_ = lb = LabelBinarizer().fit(y_train)\n",
    "\n",
    "        if subsample is not None and subsample < X_train.shape[0]:\n",
    "            X_train, y_train = shuffle(X_train, y_train, random_state=random_state)\n",
    "            X_train = X_train[:subsample]\n",
    "            y_train = y_train[:subsample]\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.Y_train = lb.transform(y_train)\n",
    "\n",
    "        if X_val is not None:\n",
    "            X_val, y_val = check_X_y(X_val, y_val, dtype=floatX)\n",
    "            if subsample is not None and subsample < X_val.shape[0]:\n",
    "                X_val, y_val = shuffle(X_val, y_val,\n",
    "                                       random_state=random_state)\n",
    "                X_val = X_val[:subsample]\n",
    "                y_val = y_val[:subsample]\n",
    "            self.Y_val = lb.transform(y_val)\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "        self.data_log_filename = data_log_filename\n",
    "        self.patience_reset = patience\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.best_cost = np.inf\n",
    "        self.patience = self.patience_reset\n",
    "    \n",
    "    def accuracy(self, y_true, output):\n",
    "        indices = output.argmax(axis=1)\n",
    "        return np.mean(self.label_binarizer_.classes_[indices] == y_true)\n",
    "        \n",
    "    def monitor(self, model, context):\n",
    "        epoch = context.get('epoch')\n",
    "        optimizer = context.get('optimizer')\n",
    "\n",
    "        training_out, training_cost = model._forward_and_cost(\n",
    "            self.X_train, self.Y_train)\n",
    "        training_acc = self.accuracy(self.y_train, training_out)\n",
    "        if self.X_val is not None:\n",
    "            validation_out, validation_cost = model._forward_and_cost(\n",
    "                self.X_val, self.Y_val)\n",
    "            validation_acc =  self.accuracy(self.y_val, validation_out)\n",
    "            current_cost = validation_cost      \n",
    "        else:\n",
    "            validation_cost = None\n",
    "            validation_acc = None\n",
    "            current_cost = training_cost\n",
    "\n",
    "        msg = \"Epoch #%03d, training cost: %0.6f, acc: %0.4f\" % (\n",
    "            epoch, training_cost, training_acc)\n",
    "        \n",
    "        if validation_out is not None:\n",
    "            msg += \", validation cost: %0.6f, acc: %0.4f\" % (\n",
    "                validation_cost, validation_acc)\n",
    "        print(msg)\n",
    "\n",
    "        # Patience-based stopping condition\n",
    "        if current_cost < self.best_cost - 1e-6:\n",
    "            self.patience = self.patience_reset\n",
    "            self.best_cost = current_cost\n",
    "        elif (hasattr(optimizer, 'on_stalled_convergence')\n",
    "              and not optimizer.on_stalled_convergence()):\n",
    "            # Optimizer has already had enough opportunities to adapt the\n",
    "            # learning rate in the past. We can start loosing patience.\n",
    "            self.patience -= 1\n",
    "        else:\n",
    "            # The optimizer does not use validation set convergence info,\n",
    "            # we can loose patience without notifying it.\n",
    "            self.patience -= 1\n",
    "        return self.patience > 0\n",
    "\n",
    "monitor = EarlyStoppingMonitor(X_train, y_train, X_val=X_val, y_val=y_val,\n",
    "                               random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, hidden=(100, 100), activation='relu', init_gain='auto',\n",
    "                 batch_size=128, optimizer=None, n_epochs=1000,\n",
    "                 weight_decay=int(1e-5), warm_start=False, random_state=None):\n",
    "        self.hidden = hidden\n",
    "        self.activation = activation\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weight_decay = weight_decay\n",
    "        self.init_gain = init_gain\n",
    "        self.warm_start = warm_start\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _init_parameters(self, n_features, n_outputs):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        input_dims = (n_features,) + self.hidden\n",
    "        output_dims = self.hidden + (n_outputs,)\n",
    "\n",
    "        if self.init_gain == 'auto':\n",
    "            g = np.sqrt(2) if self.activation == 'relu' else 1.\n",
    "        else:\n",
    "            g = self.init_gain\n",
    "        \n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        for l, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
    "            std = 2 * g / (in_dim + out_dim)\n",
    "            W = sharedX(rng.normal(size=(in_dim, out_dim), scale=std))\n",
    "            self.weights_.append(W)\n",
    "            b = sharedX(np.zeros(out_dim))\n",
    "            self.biases_.append(b)\n",
    "        \n",
    "    def _make_functions(self):\n",
    "        x = tt.matrix()\n",
    "        y = tt.matrix()\n",
    "\n",
    "        # Define the computation graph of the model\n",
    "        if self.activation == 'relu':\n",
    "            sigma = relu\n",
    "        elif self.activation == 'tanh':\n",
    "            sigma = tanh\n",
    "        elif self.activation == 'linear':\n",
    "            sigma = linear\n",
    "        else:\n",
    "            raise ValueError('Unsupported activation: %s' % self.activation)\n",
    "        \n",
    "        activations = [sigma] * (len(self.weights_) - 1) + [softmax]\n",
    "        tmp = x\n",
    "        for w, b, s in zip(self.weights_, self.biases_, activations):\n",
    "            tmp = s(tt.dot(tmp, w) + b)\n",
    "        \n",
    "        output = tmp\n",
    "        cost = tt.nnet.binary_crossentropy(output, y).mean()\n",
    "        if self.weight_decay > 0:\n",
    "            for w in self.weights_:\n",
    "                cost += self.weight_decay * tt.sqr(w).sum()\n",
    "        \n",
    "        # Use the optimizer to compute the parameter updates based\n",
    "        # on the gradient of the cost function\n",
    "        opt = self.optimizer\n",
    "        if opt is None:\n",
    "            opt = Adam()\n",
    "\n",
    "        parameters = []\n",
    "        parameters += self.weights_\n",
    "        parameters += self.biases_\n",
    "        fit_updates = opt.make_updates(parameters, cost)\n",
    "        \n",
    "        # Compile the functions them-selves\n",
    "        f = theano.function\n",
    "        self._fit = f([x, y], cost, updates=fit_updates, name='_fit')\n",
    "        self._forward_and_cost = f([x, y], (output, cost), name=\"_forward_and_cost\")\n",
    "        self._forward = f([x], output, name=\"_forward\")\n",
    "        return opt\n",
    "\n",
    "    def fit(self, X, y, monitor=None):\n",
    "        X, y = check_X_y(X, y, dtype=floatX)\n",
    "        self.label_binarizer_ = lb = LabelBinarizer()\n",
    "        Y = lb.fit_transform(y).astype(floatX)\n",
    "        n_samples, n_features = X.shape\n",
    "        _, n_outputs = Y.shape\n",
    "        if not self.warm_start or not hasattr(self, 'weights_') :\n",
    "            self._init_parameters(n_features, n_outputs)\n",
    "\n",
    "        optimizer = self._make_functions()\n",
    "        self.training_costs_ = []\n",
    "        if monitor is not None:\n",
    "            monitor.reset()\n",
    "\n",
    "        try:\n",
    "            for epoch in range(self.n_epochs):\n",
    "                if monitor is not None and not monitor.monitor(self, locals()):\n",
    "                    break\n",
    "                for X_batch, Y_batch in iter_data(X, Y):\n",
    "                    cost = self._fit(X_batch, Y_batch)\n",
    "                    self.training_costs_.append(cost)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interruped by user\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = check_array(X, dtype=theano.config.floatX)\n",
    "        return self._forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        indices = self.predict_proba(X).argmax(axis=1)\n",
    "        return self.label_binarizer_.classes_[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #000, training cost: 0.325075, acc: 0.1302, validation cost: 0.325075, acc: 0.1460\n",
      "Epoch #001, training cost: 0.084984, acc: 0.8498, validation cost: 0.089047, acc: 0.8380\n",
      "Epoch #002, training cost: 0.052269, acc: 0.9106, validation cost: 0.058006, acc: 0.9070\n",
      "Epoch #003, training cost: 0.038023, acc: 0.9386, validation cost: 0.041551, acc: 0.9340\n",
      "Epoch #004, training cost: 0.029942, acc: 0.9510, validation cost: 0.031648, acc: 0.9480\n",
      "Epoch #005, training cost: 0.025030, acc: 0.9618, validation cost: 0.026075, acc: 0.9570\n",
      "Epoch #006, training cost: 0.021554, acc: 0.9650, validation cost: 0.023162, acc: 0.9610\n",
      "Epoch #007, training cost: 0.018934, acc: 0.9692, validation cost: 0.021389, acc: 0.9650\n",
      "Epoch #008, training cost: 0.016833, acc: 0.9718, validation cost: 0.020569, acc: 0.9670\n",
      "Epoch #009, training cost: 0.014838, acc: 0.9740, validation cost: 0.019678, acc: 0.9690\n",
      "Epoch #010, training cost: 0.013268, acc: 0.9762, validation cost: 0.018941, acc: 0.9710"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = NesterovMomentumSGD(learning_rate_init=.1, learning_rate_scale=0.2, momentum=0.9)\n",
    "# optimizer = Adam()\n",
    "mlp = MLPClassifier(hidden=(100, 100), batch_size=128, init_gain='auto', weight_decay=1e-6,\n",
    "                    optimizer=optimizer, random_state=0)\n",
    "mlp.fit(X_train, y_train, monitor=monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93110000000000004"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
