{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation of a multivariate Gaussian model\n",
    "\n",
    "The goal of this notebook is to use PyTorch to implement Gradient-based MLE for a multivariate Gaussian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single 2D Gaussian component\n",
    "\n",
    "Let's generate a some data by sampling a multivariate Gaussian with an arbitrary covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "n_features = 2\n",
    "\n",
    "mean = rng.randn(n_features)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = rng.randn(n_features, n_features)\n",
    "Cov = h @ h.T\n",
    "Cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cholesky(np.linalg.inv(Cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "data = rng.multivariate_normal(mean, Cov, size=n_samples)\n",
    "plt.scatter(data[:, 0], data[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the MLE estimate from this data using the closed-form formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mle = data.mean(axis=0)\n",
    "mu_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_mle = (data - mu_mle).T @ (data - mu_mle) / n_samples\n",
    "Cov_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cholesky(np.linalg.inv(Cov_mle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrisation of a positive definite matrix\n",
    "\n",
    "\n",
    "Let's parametrize the precision matrix `P` (inverse of a covariance matrix `C`) as follows:\n",
    "\n",
    "- `P` has Cholesky decomposition `H`\n",
    "- `H` is a lower triangular with a positive diagonal\n",
    "- the log of the diagonal entry is stored in a vector of parameters named `d`\n",
    "- the off diagonal elements of `H` are stored in the matrix of parameters named `W`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "mu = Variable(torch.zeros(n_features), requires_grad=True)\n",
    "d = Variable(torch.ones(n_features), requires_grad=True)\n",
    "W = Variable(torch.randn(n_features, n_features), requires_grad=True)\n",
    "H = torch.diag(torch.exp(d)) + torch.tril(W, -1)\n",
    "P = H @ H.transpose(1, 0)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that H is the actual Cholesky decomposition of P:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cholesky(P.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P` is positive semi-definite by construction (product of a matrix `H` by its transposed).\n",
    "\n",
    "Because of we take the `exp` of `d` to build the diagonal elements of `H`, the determinant of `H` and therefore `P` is stricly positive.\n",
    "\n",
    "`P` is therefore is positive definite, whatever the values the parameters in `d` and `W`. Because the Cholesky decomposition exists for any symmetric positive-definite  matrix and is unique and `exp` is a bijection from $\\mathbb{R}$ to $\\mathbb{R}^+$, this parametrization of the manifold of positive definite matrices is bijective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(P.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(H.data.numpy()) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of `P` is cheap to compute from the `d` parameters directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.prod(torch.exp(d.data) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the above function to define the log-likelihood of a Gaussian model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "\n",
    "def loglik(X, mu, d, W):\n",
    "    \"\"\"Compute the average log-likelihood of samples\n",
    "    \n",
    "    X shape: (n_samples, n_features)\n",
    "        data points\n",
    "        \n",
    "    mu: shape: (n_features,)\n",
    "        parameters of the mean of the Gaussian.\n",
    "    \n",
    "    d: shape: (n_features,)\n",
    "        parameters of the diagonal of the Cholesky factor of the\n",
    "        precision matrix of the Gaussian.\n",
    "        \n",
    "    W: shape: (n_features, n_features)\n",
    "        parameters of the off-diagonal of the Cholesky factor of the\n",
    "        precision matrix of the Gaussian. The upper-diagonal elements\n",
    "        are ignored.\n",
    "    \"\"\"\n",
    "    H = torch.diag(torch.exp(d)) + torch.tril(W, -1)\n",
    "    P = H @ H.transpose(1, 0)\n",
    "    diff = X - mu\n",
    "    quad_form = torch.sum(diff * (diff @ P), dim=1)\n",
    "    return (-0.5 * log(2 * np.pi) + torch.sum(d) - 0.5 * quad_form)\n",
    "\n",
    "\n",
    "def nll(X, mu, d, W):\n",
    "    \"\"\"Average negative log likelihood loss to minimize\"\"\"\n",
    "    return -torch.mean(loglik(X, mu, d, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(torch.FloatTensor(data))\n",
    "loss = nll(X, mu, d, W)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward([torch.ones(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_mle = torch.FloatTensor(np.linalg.cholesky(np.linalg.inv(Cov_mle)))\n",
    "d_mle = Variable(torch.log(torch.diag(H_mle)))\n",
    "W_mle = Variable(torch.tril(H_mle, -1))\n",
    "nll(X, Variable(torch.FloatTensor(mu_mle)), d_mle, W_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = torch.optim.Adam([mu, d, W], lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # Compute and print loss.\n",
    "    loss = nll(X, mu, d, W)\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W * Variable(torch.tril(torch.ones(n_features, n_features), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of 2D Gaussian components\n",
    "\n",
    "Let's generate some ground truth Gaussian Mixture Model with 3 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "n_features = 2\n",
    "n_components = 3\n",
    "\n",
    "true_means = [rng.randn(n_features) * 3 for _ in range(n_components)]\n",
    "true_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_covariances = []\n",
    "for _ in range(n_components):\n",
    "    h = rng.randn(n_features, n_features)\n",
    "    true_covariances.append(h @ h.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some data from the ground truth model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_samples_per_component = 100\n",
    "\n",
    "samples = []\n",
    "component_ids = []\n",
    "\n",
    "for i, mean, Cov in zip(range(n_components), true_means, true_covariances):\n",
    "    data = rng.multivariate_normal(mean, Cov, size=n_samples_per_component)\n",
    "    samples.append(data)\n",
    "    component_ids.append(i * np.ones(n_samples_per_component, dtype=np.int32))\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "    \n",
    "data = np.vstack(samples)\n",
    "component_ids = np.concatenate(component_ids)\n",
    "\n",
    "data, component_ids = shuffle(data, component_ids, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no closed form formula for the MLE. Let's use the scikit-learn implementation of the EM algorithm instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "gmm_em = GaussianMixture(n_components=3, random_state=0).fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average loglikelihood of the data under the EM-MLE model (higher is better):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em.score(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(true_covariances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the MLE by gradient descent. First we need a helper function to compute the log of the sum of likelihoods of the components in a numerically stable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(data, dim=0):\n",
    "    \"\"\"Numerically stable log sum exp\"\"\"\n",
    "    max_score, _ = torch.max(data, dim)\n",
    "    if dim == 0:\n",
    "        max_score_bcast = max_score\n",
    "    elif dim == 1:\n",
    "        max_score_bcast = max_score.view(-1, 1)\n",
    "    else:\n",
    "        raise NotImplemented(\"logsumexp with dim=%d is not supported\" % dim)\n",
    "    return max_score + torch.log(torch.sum(torch.exp(data - max_score_bcast), dim))\n",
    "\n",
    "\n",
    "test_data = torch.randn(3, 4)\n",
    "logsumexp(test_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.sum(torch.exp(test_data), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Variable(torch.FloatTensor(data))\n",
    "\n",
    "\n",
    "logsotfmax = torch.nn.LogSoftmax()\n",
    "weights = Variable(torch.ones(1, n_components), requires_grad=True)\n",
    "\n",
    "means = []\n",
    "prec_diags = []\n",
    "prec_off_diags = []\n",
    "for i in range(n_components):\n",
    "    mu = Variable(torch.randn(n_features), requires_grad=True)\n",
    "    means.append(mu)\n",
    "    d = Variable(torch.ones(n_features), requires_grad=True)\n",
    "    prec_diags.append(d)\n",
    "    W = Variable(torch.randn(n_features, n_features), requires_grad=True)\n",
    "    prec_off_diags.append(W)\n",
    "\n",
    "\n",
    "def mixture_nll(X, weights, means, prec_diags, prec_off_diags):\n",
    "    log_normalized_weights = logsotfmax(weights).transpose(1, 0)\n",
    "    logliks = []\n",
    "    for mu, d, W in zip(means, prec_diags, prec_off_diags):\n",
    "        logliks.append(loglik(X, mu, d, W))\n",
    "    \n",
    "    logliks = torch.cat(logliks).view(n_components, -1)\n",
    "    return torch.mean(-logsumexp(logliks + log_normalized_weights, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_nll(X, weights, means, prec_diags, prec_off_diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [weights]\n",
    "params.extend(means)\n",
    "params.extend(prec_diags)\n",
    "params.extend(prec_off_diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "best_loss = np.inf\n",
    "for t in range(5000):\n",
    "    loss = mixture_nll(X, weights, means, prec_diags, prec_off_diags)\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss.data[0])\n",
    "    if loss.data.numpy() < best_loss:\n",
    "        best_loss = loss.data.numpy()\n",
    "    else:\n",
    "        print(t, loss.data[0], 'converged!')\n",
    "        break\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[m.data.numpy() for m in means]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax()\n",
    "softmax(weights).view(-1).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_em.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
