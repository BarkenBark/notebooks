{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import shuffle\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "from theano import shared\n",
    "from time import time\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Creating train-test split...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "def load_data(dtype=np.float32, order='C'):\n",
    "    \"\"\"Load the data, then cache and memmap the train/test split\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    data = fetch_mldata('MNIST original')\n",
    "    X = check_array(data['data'], dtype=dtype, order=order)\n",
    "    y = data[\"target\"]\n",
    "    # Scale the features to [0, 1]\n",
    "    X = X / 255\n",
    "    ## Create train-test split (as [Joachims, 2006])\n",
    "    print(\"Creating train-test split...\")\n",
    "    n_train = 60000\n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = load_data()\n",
    "\n",
    "# Create a small training set for faster experiments\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_dev, y_dev, test_size=int(1e3), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various neural networks utilities\n",
    "\n",
    "Mostly taken from Alec Radford and Kyle Kastner's gists:\n",
    "\n",
    "- https://gist.github.com/Newmu/a56d5446416f5ad2bbac\n",
    "- https://gist.github.com/kastnerkyle/f3f67424adda343fef40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sharedf32(X, dtype=np.float32, name=None):\n",
    "    return theano.shared(np.asarray(X, dtype=dtype), name=name)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return tt.nnet.softmax(x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return (x + abs(x)) / 2.0\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return tt.tanh(x)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return tt.nnet.sigmoid(x)\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def iter_data(*data, **kwargs):\n",
    "    batch_size = kwargs.get('batch_size', 128)\n",
    "    batches, remainder = divmod(len(data[0]), batch_size)\n",
    "    if remainder != 0:\n",
    "        batches += 1\n",
    "    for b in range(batches):\n",
    "        start = b * batch_size\n",
    "        end = (b + 1) * batch_size\n",
    "        if len(data) == 1:\n",
    "            yield data[0][start:end]\n",
    "        else:\n",
    "            yield tuple([d[start:end] for d in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    \"\"\"no bias init correction and no b1 decay\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, grad_momentum=0.1,\n",
    "                 sq_grad_momentum=0.001, eps=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_momentum = grad_momentum\n",
    "        self.sq_grad_momentum = sq_grad_momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def make_updates(self, params, cost):\n",
    "        b1 = self.grad_momentum\n",
    "        b2 = self.sq_grad_momentum\n",
    "        updates = []\n",
    "        grads = tt.grad(cost, params)\n",
    "        for p, g in zip(params, grads):\n",
    "            m = sharedf32(p.get_value() * 0., name='m')\n",
    "            v = sharedf32(p.get_value() * 0., name='v')\n",
    "            m_t = (b1 * g) + ((1. - b1) * m)\n",
    "            v_t = (b2 * tt.sqr(g)) + ((1. - b2) * v)\n",
    "            g_t = m_t / (tt.sqrt(v_t) + self.eps)\n",
    "            p_t = p - self.learning_rate * g_t\n",
    "            updates.append((m, m_t))\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((p, p_t))\n",
    "        return updates\n",
    "    \n",
    "    \n",
    "class MomentumSGD(object):\n",
    "    \n",
    "    monitor = None\n",
    "    \n",
    "    def __init__(self, learning_rate_init=0.1, max_adaptation_count=3,\n",
    "                 learning_rate_scale=0.2, momentum=0.9):\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_adaptation_count = max_adaptation_count\n",
    "        self.learning_rate_scale = learning_rate_scale\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def on_stalled_convergence(self):\n",
    "        if self.adaptation_count > self.max_adaptation_count:\n",
    "            # Optimizer thinks model has converged\n",
    "            return False\n",
    "        \n",
    "        scale = np.array(self.learning_rate_scale, dtype=np.float32)\n",
    "        \n",
    "        # Rescale the learning rate\n",
    "        old_lr = self.learning_rate.get_value()\n",
    "        new_lr = old_lr * scale\n",
    "        self.learning_rate.set_value(new_lr)\n",
    "        self.adaptation_count += 1\n",
    "        print(\"Updated learning rate from %f to %f\" % (old_lr, new_lr))\n",
    "\n",
    "    def make_updates(self, params, cost):\n",
    "        self.adaptation_count = 0\n",
    "        self.learning_rate = sharedf32(self.learning_rate_init,\n",
    "                                       name='learning_rate')\n",
    "        momentum = np.array(self.momentum, dtype=np.float32)\n",
    "\n",
    "        updates = []\n",
    "        grads = tt.grad(cost, params)\n",
    "        for p, g in zip(params, grads):\n",
    "            if self.momentum > 0:\n",
    "                v = sharedf32(p.get_value() * 0., name='v')\n",
    "                v_t = momentum * v - self.learning_rate * g\n",
    "                p_t = p + v_t\n",
    "                updates.append((v, v_t))\n",
    "            else:\n",
    "                # traditional SGD\n",
    "                p_t = p - self.learning_rate * g\n",
    "            updates.append((p, p_t))\n",
    "        return updates\n",
    "\n",
    "    \n",
    "class NesterovMomentumSGD(MomentumSGD):\n",
    "\n",
    "    def make_updates(self, params, cost):\n",
    "        self.adaptation_count = 0\n",
    "        self.learning_rate = sharedf32(self.learning_rate_init,\n",
    "                                       name='learning_rate')\n",
    "        momentum = np.array(self.momentum, dtype=np.float32)\n",
    "        updates = []\n",
    "        grads = tt.grad(cost, params)\n",
    "        for p, g in zip(params, grads):\n",
    "            v = sharedf32(p.get_value() * 0., name='v')\n",
    "            v_t = momentum * v - self.learning_rate * g\n",
    "            p_t = p + momentum * v_t - self.learning_rate * g\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((p, p_t))\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingMonitor(object):\n",
    "    def __init__(self, X_train, y_train, X_val=None, y_val=None, patience=3,\n",
    "                 data_log_filename=None, subsample=int(5e3), random_state=None):\n",
    "\n",
    "        X_train, y_train = check_X_y(X_train, y_train, dtype=np.float32)\n",
    "        self.label_binarizer_ = lb = LabelBinarizer().fit(y_train)\n",
    "\n",
    "        if subsample is not None and subsample < X_train.shape[0]:\n",
    "            X_train, y_train = shuffle(X_train, y_train, random_state=random_state)\n",
    "            X_train = X_train[:subsample]\n",
    "            y_train = y_train[:subsample]\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.Y_train = lb.transform(y_train).astype(np.float32)\n",
    "\n",
    "        if X_val is not None:\n",
    "            X_val, y_val = check_X_y(X_val, y_val, dtype=np.float32)\n",
    "            if subsample is not None and subsample < X_val.shape[0]:\n",
    "                X_val, y_val = shuffle(X_val, y_val,\n",
    "                                       random_state=random_state)\n",
    "                X_val = X_val[:subsample]\n",
    "                y_val = y_val[:subsample]\n",
    "            self.Y_val = lb.transform(y_val).astype(np.float32)\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "        self.data_log_filename = data_log_filename\n",
    "        self.patience_reset = patience\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.best_cost = np.inf\n",
    "        self.patience = self.patience_reset\n",
    "    \n",
    "    def accuracy(self, y_true, output):\n",
    "        indices = output.argmax(axis=1)\n",
    "        return np.mean(self.label_binarizer_.classes_[indices] == y_true)\n",
    "        \n",
    "    def monitor(self, model, context):\n",
    "        epoch = context.get('epoch')\n",
    "        optimizer = context.get('optimizer')\n",
    "\n",
    "        training_out, training_cost = model._forward_and_cost(\n",
    "            self.X_train, self.Y_train)\n",
    "        training_acc = self.accuracy(self.y_train, training_out)\n",
    "        if self.X_val is not None:\n",
    "            validation_out, validation_cost = model._forward_and_cost(\n",
    "                self.X_val, self.Y_val)\n",
    "            validation_acc =  self.accuracy(self.y_val, validation_out)\n",
    "            current_cost = validation_cost      \n",
    "        else:\n",
    "            validation_cost = None\n",
    "            validation_acc = None\n",
    "            current_cost = training_cost\n",
    "\n",
    "        msg = \"Epoch #%03d, training cost: %0.6f, acc: %0.4f\" % (\n",
    "            epoch, training_cost, training_acc)\n",
    "        \n",
    "        if validation_out is not None:\n",
    "            msg += \", validation cost: %0.6f, acc: %0.4f\" % (\n",
    "                validation_cost, validation_acc)\n",
    "        print(msg)\n",
    "\n",
    "        # Patience-based stopping condition\n",
    "        if current_cost < self.best_cost - 1e-6:\n",
    "            self.patience = self.patience_reset\n",
    "            self.best_cost = current_cost\n",
    "        elif (hasattr(optimizer, 'on_stalled_convergence')\n",
    "              and not optimizer.on_stalled_convergence()):\n",
    "            # Optimizer has already had enough opportunities to adapt the\n",
    "            # learning rate in the past. We can start loosing patience.\n",
    "            self.patience -= 1\n",
    "        else:\n",
    "            # The optimizer does not use validation set convergence info,\n",
    "            # we can loose patience without notifying it.\n",
    "            self.patience -= 1\n",
    "        return self.patience > 0\n",
    "\n",
    "monitor = EarlyStoppingMonitor(X_train, y_train, X_val=X_val, y_val=y_val,\n",
    "                               random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, hidden=(100, 100), activation='relu', init_gain='auto',\n",
    "                 batch_size=128, optimizer=None, n_epochs=1000,\n",
    "                 weight_decay=int(1e-5), warm_start=False, random_state=None):\n",
    "        self.hidden = hidden\n",
    "        self.activation = activation\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weight_decay = weight_decay\n",
    "        self.init_gain = init_gain\n",
    "        self.warm_start = warm_start\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _init_parameters(self, n_features, n_outputs):\n",
    "        rng = check_random_state(self.random_state)\n",
    "        input_dims = (n_features,) + self.hidden\n",
    "        output_dims = self.hidden + (n_outputs,)\n",
    "\n",
    "        if self.init_gain == 'auto':\n",
    "            g = np.sqrt(2) if self.activation == 'relu' else 1.\n",
    "        else:\n",
    "            g = self.init_gain\n",
    "        \n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        for l, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
    "            std = 2 * g / (in_dim + out_dim)\n",
    "            W = sharedf32(rng.normal(size=(in_dim, out_dim), scale=std),\n",
    "                          name='W_%d' % l)\n",
    "            self.weights_.append(W)\n",
    "            b = sharedf32(np.zeros(out_dim), name='b_%d' % l)\n",
    "            self.biases_.append(b)\n",
    "        \n",
    "    def _make_functions(self):\n",
    "        x = tt.fmatrix(name='x')\n",
    "        y = tt.fmatrix(name='y')\n",
    "\n",
    "        # Define the computation graph of the model\n",
    "        if self.activation == 'relu':\n",
    "            sigma = relu\n",
    "        elif self.activation == 'tanh':\n",
    "            sigma = tanh\n",
    "        elif self.activation == 'linear':\n",
    "            sigma = linear\n",
    "        else:\n",
    "            raise ValueError('Unsupported activation: %s' % self.activation)\n",
    "        \n",
    "        activations = [sigma] * (len(self.weights_) - 1) + [softmax]\n",
    "        tmp = x\n",
    "        for w, b, s in zip(self.weights_, self.biases_, activations):\n",
    "            tmp = s(tt.dot(tmp, w) + b)\n",
    "        \n",
    "        output = tmp\n",
    "        cost = tt.nnet.binary_crossentropy(output, y).mean()\n",
    "        weight_decay = np.array(self.weight_decay, dtype=np.float32)\n",
    "        if self.weight_decay > 0:\n",
    "            for w in self.weights_:\n",
    "                cost += weight_decay * tt.sqr(w).sum()\n",
    "        \n",
    "        # Use the optimizer to compute the parameter updates based\n",
    "        # on the gradient of the cost function\n",
    "        opt = self.optimizer\n",
    "        if opt is None:\n",
    "            opt = Adam()\n",
    "\n",
    "        parameters = []\n",
    "        parameters += self.weights_\n",
    "        parameters += self.biases_\n",
    "        fit_updates = opt.make_updates(parameters, cost)\n",
    "        \n",
    "        # Compile the functions them-selves\n",
    "        f = theano.function\n",
    "        self._fit = f([x, y], cost, updates=fit_updates, name='_fit')\n",
    "        self._forward_and_cost = f([x, y], (output, cost), name=\"_forward_and_cost\")\n",
    "        self._forward = f([x], output, name=\"_forward\")\n",
    "        return opt\n",
    "\n",
    "    def fit(self, X, y, monitor=None):\n",
    "        X, y = check_X_y(X, y, dtype=np.float32)\n",
    "        self.label_binarizer_ = lb = LabelBinarizer()\n",
    "        Y = lb.fit_transform(y).astype(np.float32)\n",
    "        n_samples, n_features = X.shape\n",
    "        _, n_outputs = Y.shape\n",
    "        if not self.warm_start or not hasattr(self, 'weights_') :\n",
    "            self._init_parameters(n_features, n_outputs)\n",
    "\n",
    "        optimizer = self._make_functions()\n",
    "        self.training_costs_ = []\n",
    "        if monitor is not None:\n",
    "            monitor.reset()\n",
    "\n",
    "        try:\n",
    "            for epoch in range(self.n_epochs):\n",
    "                if monitor is not None and not monitor.monitor(self, locals()):\n",
    "                    break\n",
    "                for X_batch, Y_batch in iter_data(X, Y):\n",
    "                    cost = self._fit(X_batch, Y_batch)\n",
    "                    self.training_costs_.append(cost)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interruped by user\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = check_array(X, dtype=np.float32)\n",
    "        return self._forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        indices = self.predict_proba(X).argmax(axis=1)\n",
    "        return self.label_binarizer_.classes_[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #000, training cost: 0.325075, acc: 0.1302, validation cost: 0.325075, acc: 0.1460\n",
      "Epoch #001, training cost: 0.324683, acc: 0.1044, validation cost: 0.324515, acc: 0.1200\n",
      "Epoch #002, training cost: 0.323131, acc: 0.2064, validation cost: 0.322861, acc: 0.2160\n",
      "Epoch #003, training cost: 0.305443, acc: 0.2696, validation cost: 0.305597, acc: 0.2700\n",
      "Epoch #004, training cost: 0.257029, acc: 0.4432, validation cost: 0.258248, acc: 0.4350\n",
      "Epoch #005, training cost: 0.153836, acc: 0.7136, validation cost: 0.155097, acc: 0.7050\n",
      "Epoch #006, training cost: 0.106109, acc: 0.8028, validation cost: 0.107787, acc: 0.7970\n",
      "Epoch #007, training cost: 0.087124, acc: 0.8440, validation cost: 0.089504, acc: 0.8440\n",
      "Epoch #008, training cost: 0.076055, acc: 0.8678, validation cost: 0.078921, acc: 0.8670\n",
      "Epoch #009, training cost: 0.069002, acc: 0.8818, validation cost: 0.072658, acc: 0.8750\n",
      "Epoch #010, training cost: 0.063858, acc: 0.8906, validation cost: 0.068460, acc: 0.8870\n",
      "Epoch #011, training cost: 0.059908, acc: 0.8960, validation cost: 0.065099, acc: 0.8960\n",
      "Epoch #012, training cost: 0.056747, acc: 0.9018, validation cost: 0.062131, acc: 0.9060\n",
      "Epoch #013, training cost: 0.054007, acc: 0.9070, validation cost: 0.059333, acc: 0.9090\n",
      "Epoch #014, training cost: 0.051519, acc: 0.9126, validation cost: 0.056641, acc: 0.9110\n",
      "Epoch #015, training cost: 0.049198, acc: 0.9172, validation cost: 0.054057, acc: 0.9170\n",
      "Epoch #016, training cost: 0.046986, acc: 0.9222, validation cost: 0.051561, acc: 0.9170\n",
      "Epoch #017, training cost: 0.044860, acc: 0.9262, validation cost: 0.049019, acc: 0.9200\n",
      "Epoch #018, training cost: 0.042848, acc: 0.9302, validation cost: 0.046604, acc: 0.9210\n",
      "Epoch #019, training cost: 0.040930, acc: 0.9326, validation cost: 0.044350, acc: 0.9250\n",
      "Epoch #020, training cost: 0.039113, acc: 0.9356, validation cost: 0.042199, acc: 0.9290\n",
      "Epoch #021, training cost: 0.037434, acc: 0.9370, validation cost: 0.040217, acc: 0.9340\n",
      "Epoch #022, training cost: 0.035893, acc: 0.9406, validation cost: 0.038443, acc: 0.9370\n",
      "Epoch #023, training cost: 0.034480, acc: 0.9438, validation cost: 0.036859, acc: 0.9380\n",
      "Epoch #024, training cost: 0.033197, acc: 0.9456, validation cost: 0.035456, acc: 0.9400\n",
      "Epoch #025, training cost: 0.032024, acc: 0.9484, validation cost: 0.034171, acc: 0.9410\n",
      "Epoch #026, training cost: 0.030946, acc: 0.9504, validation cost: 0.032976, acc: 0.9470\n",
      "Epoch #027, training cost: 0.029948, acc: 0.9530, validation cost: 0.031900, acc: 0.9490\n",
      "Epoch #028, training cost: 0.029027, acc: 0.9552, validation cost: 0.030928, acc: 0.9490\n",
      "Epoch #029, training cost: 0.028167, acc: 0.9566, validation cost: 0.030058, acc: 0.9480\n",
      "Epoch #030, training cost: 0.027364, acc: 0.9584, validation cost: 0.029276, acc: 0.9480\n",
      "Epoch #031, training cost: 0.026612, acc: 0.9596, validation cost: 0.028576, acc: 0.9490\n",
      "Epoch #032, training cost: 0.025897, acc: 0.9616, validation cost: 0.027945, acc: 0.9520\n",
      "Epoch #033, training cost: 0.025233, acc: 0.9622, validation cost: 0.027360, acc: 0.9530\n",
      "Epoch #034, training cost: 0.024600, acc: 0.9632, validation cost: 0.026797, acc: 0.9540\n",
      "Epoch #035, training cost: 0.023997, acc: 0.9642, validation cost: 0.026294, acc: 0.9540\n",
      "Epoch #036, training cost: 0.023427, acc: 0.9652, validation cost: 0.025839, acc: 0.9560\n",
      "Epoch #037, training cost: 0.022878, acc: 0.9652, validation cost: 0.025434, acc: 0.9560\n",
      "Epoch #038, training cost: 0.022365, acc: 0.9664, validation cost: 0.025045, acc: 0.9570\n",
      "Epoch #039, training cost: 0.021866, acc: 0.9672, validation cost: 0.024706, acc: 0.9570\n",
      "Epoch #040, training cost: 0.021391, acc: 0.9672, validation cost: 0.024383, acc: 0.9580\n",
      "Epoch #041, training cost: 0.020933, acc: 0.9684, validation cost: 0.024072, acc: 0.9580\n",
      "Epoch #042, training cost: 0.020502, acc: 0.9690, validation cost: 0.023756, acc: 0.9580\n",
      "Epoch #043, training cost: 0.020059, acc: 0.9694, validation cost: 0.023493, acc: 0.9590\n",
      "Epoch #044, training cost: 0.019642, acc: 0.9700, validation cost: 0.023226, acc: 0.9590\n",
      "Epoch #045, training cost: 0.019225, acc: 0.9704, validation cost: 0.022955, acc: 0.9600\n",
      "Epoch #046, training cost: 0.018824, acc: 0.9710, validation cost: 0.022683, acc: 0.9580\n",
      "Epoch #047, training cost: 0.018436, acc: 0.9724, validation cost: 0.022426, acc: 0.9590\n",
      "Epoch #048, training cost: 0.018051, acc: 0.9726, validation cost: 0.022155, acc: 0.9600\n",
      "Epoch #049, training cost: 0.017682, acc: 0.9728, validation cost: 0.021881, acc: 0.9600\n",
      "Epoch #050, training cost: 0.017317, acc: 0.9732, validation cost: 0.021622, acc: 0.9600\n",
      "Epoch #051, training cost: 0.016950, acc: 0.9740, validation cost: nan, acc: 0.9600\n",
      "Updated learning rate from 0.010000 to 0.002000\n",
      "Epoch #052, training cost: nan, acc: 0.1056, validation cost: nan, acc: 0.0990\n",
      "Updated learning rate from 0.002000 to 0.000400\n",
      "Epoch #053, training cost: nan, acc: 0.1056, validation cost: nan, acc: 0.0990\n",
      "Updated learning rate from 0.000400 to 0.000080\n",
      "CPU times: user 1min 31s, sys: 4.37 s, total: 1min 36s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = NesterovMomentumSGD(learning_rate_init=.01, learning_rate_scale=0.2, momentum=0.9)\n",
    "# optimizer = Adam()\n",
    "mlp = MLPClassifier(hidden=(100, 100), batch_size=128, init_gain='auto', weight_decay=1e-6,\n",
    "                    optimizer=optimizer, random_state=0)\n",
    "mlp.fit(X_train, y_train, monitor=monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.098000000000000004"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
