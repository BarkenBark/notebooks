{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from os.path import expanduser, join\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "\n",
      "from sklearn.externals import joblib\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "from pyrallel.ensemble import EnsembleGrower\n",
      "from pyrallel.ensemble import sub_ensemble"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "lb_view = Client().load_balanced_view()\n",
      "len(lb_view)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "31"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading the dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a NumPy array version of Fold1 of the [MSLR-WEB10K](http://research.microsoft.com/en-us/projects/mslr/) dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "data = np.load(expanduser('~/data/MSLR-WEB10K/mslr-web10k_fold1.npz'))\n",
      "X_train, y_train, qid_train = data['X_train'], data['y_train'], data['qid_train']\n",
      "X_vali, y_vali, qid_vali = data['X_vali'], data['y_vali'], data['qid_vali']\n",
      "X_test, y_test, qid_test = data['X_test'], data['y_test'], data['qid_test']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 3.38 s, sys: 1.04 s, total: 4.42 s\n",
        "Wall time: 4.41 s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python2.7/dist-packages/numpy/lib/utils.py:1132: DeprecationWarning: The compiler package is deprecated and removed in Python 3.x.\n",
        "  import compiler\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(X_train.nbytes + X_vali.nbytes + X_test.nbytes) / 1e6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "652.904448"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape, X_train.dtype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "((723412, 136), dtype('float32'))"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_qid_train = np.unique(qid_train)\n",
      "len(unique_qid_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "6000"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extract a subset of 500 queries to speed up the learning when prototyping"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rng = np.random.RandomState(0)\n",
      "qid_mask = rng.permutation(len(unique_qid_train))[:500]\n",
      "subset_mask = np.in1d(qid_train, unique_qid_train[qid_mask])\n",
      "X_train_small = X_train[subset_mask]\n",
      "y_train_small = y_train[subset_mask]\n",
      "qid_train_small = qid_train[subset_mask]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_small.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "(62244, 136)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sanity check:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(np.unique(qid_train_small))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "500"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Quantifying ranking success with NDCG"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dcg(relevances, rank=10):\n",
      "    \"\"\"Discounted cumulative gain at rank (DCG)\"\"\"\n",
      "    relevances = np.asarray(relevances)[:rank]\n",
      "    n_relevances = len(relevances)\n",
      "    if n_relevances == 0:\n",
      "        return 0.\n",
      "\n",
      "    discounts = np.log2(np.arange(n_relevances) + 2)\n",
      "    return np.sum(relevances / discounts)\n",
      " \n",
      " \n",
      "def ndcg(relevances, rank=10):\n",
      "    \"\"\"Normalized discounted cumulative gain (NDGC)\"\"\"\n",
      "    best_dcg = dcg(sorted(relevances, reverse=True), rank)\n",
      "    if best_dcg == 0:\n",
      "        return 0.\n",
      "\n",
      "    return dcg(relevances, rank) / best_dcg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndcg([2, 4, 0, 1, 1, 0, 0], rank=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.86253003992915656"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndcg([0, 0, 0, 1, 1, 2, 4], rank=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "0.13201850690866795"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndcg([0, 0, 0, 1, 1, 2, 4], rank=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "0.0"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndcg([4, 2, 1, 1, 0, 0, 0], rank=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mean_ndcg(y_true, y_pred, query_ids, rank=10):\n",
      "    y_true = np.asarray(y_true)\n",
      "    y_pred = np.asarray(y_pred)\n",
      "    query_ids = np.asarray(query_ids)\n",
      "    # assume query_ids are sorted\n",
      "    ndcg_scores = []\n",
      "    previous_qid = query_ids[0]\n",
      "    previous_loc = 0\n",
      "    for loc, qid in enumerate(query_ids):\n",
      "        if previous_qid != qid:\n",
      "            chunk = slice(previous_loc, loc)\n",
      "            ranked_relevances = y_true[chunk][np.argsort(y_pred[chunk])[::-1]]\n",
      "            ndcg_scores.append(ndcg(ranked_relevances, rank=rank))\n",
      "            previous_loc = loc\n",
      "        previous_qid = qid\n",
      "\n",
      "    chunk = slice(previous_loc, loc + 1)\n",
      "    ranked_relevances = y_true[chunk][np.argsort(y_pred[chunk])[::-1]]\n",
      "    ndcg_scores.append(ndcg(ranked_relevances, rank=rank))\n",
      "    return np.mean(ndcg_scores)\n",
      "\n",
      "\n",
      "mean_ndcg([4, 3, 1, 4, 3], [4, 0, 1, 4, 2], [0, 0, 0, 2, 2], rank=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.9795191506818377"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Growing Randomized Trees to predict relevance scores"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grower = EnsembleGrower(lb_view, ExtraTreesRegressor(n_estimators=1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grower.launch(X_train, y_train, n_estimators=500, folder=\"mlsr-web10k\", name='etr')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "Progress: 00% (000/500), elapsed: 1.006s\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grower"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "Progress: 25% (127/500), elapsed: 1312.188s\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#grower.wait()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time etr = grower.aggregate_model()\n",
      "print(\"number of trees: {}\".format(len(etr.estimators_)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 3.28 s, sys: 1.3 s, total: 4.58 s\n",
        "Wall time: 4.58 s\n",
        "number of trees: 127\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time y_vali_etr = etr.predict(X_vali)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"NDCG_3 score: {:.3f}\".format(\n",
      "    mean_ndcg(y_vali, y_vali_etr, qid_vali, rank=3)))\n",
      "print(\"NDCG_10 score: {:.3f}\".format(\n",
      "    mean_ndcg(y_vali, y_vali_etr, qid_vali, rank=10)))\n",
      "print(\"R2 score: {:.3f}\".format(r2_score(y_vali, y_vali_etr)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Impact of the number of trees in the ensemble"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "n_runs = 5\n",
      "max_n_trees = len(etr.estimators_)\n",
      "n_trees = np.linspace(1, max_n_trees, 5).astype(int)\n",
      "\n",
      "scores = np.zeros((n_runs, len(n_trees)), dtype=np.float)\n",
      "\n",
      "for i in range(n_runs):\n",
      "    for j, n in enumerate(n_trees):\n",
      "        y_predicted = sub_ensemble(etr, n, seed=i + j).predict(X_vali)\n",
      "        scores[i, j] = mean_ndcg(y_vali, y_predicted, qid_vali, rank=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(n_trees, scores.mean(axis=0))\n",
      "plt.boxplot(scores, positions=n_trees)\n",
      "plt.xlabel(\"Number of trees\")\n",
      "plt.ylabel(\"Average NDC@10\")\n",
      "_ = plt.title(\"Impact of the number of trees\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Comparing with a baseline linear regression model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time lr = LinearRegression().fit(X_train_small, y_train_small)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time y_vali_lr = lr.predict(X_vali)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"NDCG_3 score: {:.3f}\".format(\n",
      "    mean_ndcg(y_vali, y_vali_lr, qid_vali, rank=3)))\n",
      "print(\"NDCG_10 score: {:.3f}\".format(\n",
      "    mean_ndcg(y_vali, y_vali_lr, qid_vali, rank=10)))\n",
      "print(\"R2 score: {:.3f}\".format(r2_score(y_vali, y_vali_lr)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluate overfitting by comparing with training set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_small_lr = lr.predict(X_train_small)\n",
      "\n",
      "print(\"NDCG_3 score: {:.3f}\".format(\n",
      "    mean_ndcg(y_train_small, y_train_small_lr, qid_train_small, rank=3)))\n",
      "print(\"NDCG_10 score: {:.3f}\".format(\n",
      "    mean_ndcg(y_train_small, y_train_small_lr, qid_train_small, rank=10)))\n",
      "print(\"R2 score: {:.3f}\".format(r2_score(y_train_small, y_train_small_lr)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interestingly enough, a overfitting of the training set from a regression standpoint (higher r2 score) does not seem to cause overfitting from a ranking standpoint. This would have to be checked with cross-validation though."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introspecting the distribution of relevance scores predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subset = np.random.permutation(y_vali.shape[0])[:10000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.title('Extra Trees predictions')\n",
      "plt.scatter(y_vali[subset], y_vali_etr[subset], alpha=0.01, s=100)\n",
      "plt.xlabel('True relevance')\n",
      "plt.ylabel('Predicted relevance')\n",
      "plt.ylim(-2, 5)\n",
      "plt.xlim(-2, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.title('Linear Regression predictions')\n",
      "plt.scatter(y_vali[subset], y_vali_lr[subset], alpha=0.01, s=100)\n",
      "plt.xlabel('True relevance')\n",
      "plt.ylabel('Predicted relevance')\n",
      "plt.ylim(-2, 5)\n",
      "plt.xlim(-2, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(y_vali, bins=5, alpha=.3, color='b', label='true relevance')\n",
      "plt.hist(y_vali_etr, bins=5, alpha=.3, color='g', label='ET predicted relevance')\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each query, count the number of results with rank 0, 1, 2, 3 or 4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_qids_vali = np.unique(qid_vali)\n",
      "for qid in unique_qids_vali[:10]:\n",
      "    qids = y_vali[qid_vali == qid].astype(np.int)\n",
      "    print(np.bincount(qids, minlength=5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}