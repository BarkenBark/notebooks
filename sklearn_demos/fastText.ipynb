{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks is an experiment to see if a pure scikit-learn implementation of the fastText model can work better than a linear model on a small text classification problem: 20 newsgroups.\n",
    "\n",
    "http://arxiv.org/abs/1607.01759\n",
    "\n",
    "Those models are very similar to Deep Averaging Network (with only 1 hidden layer with a linear activation function):\n",
    "\n",
    "https://www.cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf\n",
    "\n",
    "\n",
    "Note that scikit-learn does not provide a hierarchical softmax implementation (but we don't need it on 20 newsgroups anyways)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twentyng_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    #remove=('headers', 'footers'),\n",
    ")\n",
    "docs_train, target_train = twentyng_train.data, twentyng_train.target\n",
    "\n",
    "\n",
    "twentyng_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    #remove=('headers', 'footers'),\n",
    ")\n",
    "\n",
    "docs_test, target_test = twentyng_test.data, twentyng_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following uses the hashing tricks on unigrams and bigrams. `binary=True` makes us ignore repeated words in a document. The `l1` normalization ensures that we \"average\" the embeddings of the tokens in the document instead of summing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 116 ms, total: 16.9 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = HashingVectorizer(\n",
    "    encoding='latin-1', binary=True, ngram_range=(1, 2),\n",
    "    norm='l1', n_features=2 ** 18)\n",
    "\n",
    "X_train = vec.transform(docs_train)\n",
    "X_test = vec.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc_vectors = X_train[:3].toarray()\n",
    "first_doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc_vectors.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0049505 ,  0.00469484,  0.00200401])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc_vectors.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc_vectors.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline: OvR logistic regression (the multinomial logistic regression loss is currently not implemented in scikit-learn). In practice, the OvR reduction seems to work well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.3 s, sys: 3.37 s, total: 54.7 s\n",
      "Wall time: 5.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log', alpha=1e-10, n_iter=30, n_jobs=-1)\n",
    "lr.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score: 0.823\n",
      "CPU times: user 289 ms, sys: 740 ms, total: 1.03 s\n",
      "Wall time: 295 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"test score: %0.3f\" % lr.score(X_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the MLPClassifier of scikit-learn to add a single hidden layer with a small number of hidden units.\n",
    "\n",
    "Note: instead of tanh or relu we would rather like to use a linear / identity activation function for the hidden layer but this is not (yet) implemented in scikit-learn.\n",
    "\n",
    "In that respect the following model is closer to a Deep Averaging Network (without dropout) than fastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.00852041\n",
      "Iteration 2, loss = 2.98454154\n",
      "Iteration 3, loss = 2.95940918\n",
      "Iteration 4, loss = 2.93309926\n",
      "Iteration 5, loss = 2.90229842\n",
      "Iteration 6, loss = 2.86498372\n",
      "Iteration 7, loss = 2.82006721\n",
      "Iteration 8, loss = 2.76721310\n",
      "Iteration 9, loss = 2.70666565\n",
      "Iteration 10, loss = 2.63868516\n",
      "Iteration 11, loss = 2.56438989\n",
      "Iteration 12, loss = 2.48427410\n",
      "Iteration 13, loss = 2.39930013\n",
      "Iteration 14, loss = 2.31064855\n",
      "Iteration 15, loss = 2.21980731\n",
      "Iteration 16, loss = 2.12689619\n",
      "Iteration 17, loss = 2.03369668\n",
      "Iteration 18, loss = 1.94064003\n",
      "Iteration 19, loss = 1.84855330\n",
      "Iteration 20, loss = 1.75839564\n",
      "Iteration 21, loss = 1.67029382\n",
      "Iteration 22, loss = 1.58553153\n",
      "Iteration 23, loss = 1.50368438\n",
      "Iteration 24, loss = 1.42559216\n",
      "Iteration 25, loss = 1.35092920\n",
      "Iteration 26, loss = 1.28031093\n",
      "Iteration 27, loss = 1.21336656\n",
      "Iteration 28, loss = 1.15018003\n",
      "Iteration 29, loss = 1.09083649\n",
      "Iteration 30, loss = 1.03492320\n",
      "Iteration 31, loss = 0.98235703\n",
      "Iteration 32, loss = 0.93320371\n",
      "Iteration 33, loss = 0.88714760\n",
      "Iteration 34, loss = 0.84397938\n",
      "Iteration 35, loss = 0.80385285\n",
      "Iteration 36, loss = 0.76627057\n",
      "Iteration 37, loss = 0.73104698\n",
      "Iteration 38, loss = 0.69816271\n",
      "Iteration 39, loss = 0.66761124\n",
      "Iteration 40, loss = 0.63904126\n",
      "Iteration 41, loss = 0.61253583\n",
      "Iteration 42, loss = 0.58763096\n",
      "Iteration 43, loss = 0.56439202\n",
      "Iteration 44, loss = 0.54266013\n",
      "Iteration 45, loss = 0.52247408\n",
      "Iteration 46, loss = 0.50363623\n",
      "Iteration 47, loss = 0.48576569\n",
      "Iteration 48, loss = 0.46932127\n",
      "Iteration 49, loss = 0.45377693\n",
      "Iteration 50, loss = 0.43922204\n",
      "Iteration 51, loss = 0.42567346\n",
      "Iteration 52, loss = 0.41285229\n",
      "Iteration 53, loss = 0.40092168\n",
      "Iteration 54, loss = 0.38965003\n",
      "Iteration 55, loss = 0.37903688\n",
      "Iteration 56, loss = 0.36904403\n",
      "Iteration 57, loss = 0.35967555\n",
      "Iteration 58, loss = 0.35080347\n",
      "Iteration 59, loss = 0.34242944\n",
      "Iteration 60, loss = 0.33453168\n",
      "Iteration 61, loss = 0.32702134\n",
      "Iteration 62, loss = 0.32000339\n",
      "Iteration 63, loss = 0.31324668\n",
      "Iteration 64, loss = 0.30685281\n",
      "Iteration 65, loss = 0.30083586\n",
      "Iteration 66, loss = 0.29510026\n",
      "Iteration 67, loss = 0.28959667\n",
      "Iteration 68, loss = 0.28433985\n",
      "Iteration 69, loss = 0.27935700\n",
      "Iteration 70, loss = 0.27454894\n",
      "Iteration 71, loss = 0.27001176\n",
      "Iteration 72, loss = 0.26559624\n",
      "Iteration 73, loss = 0.26139336\n",
      "Iteration 74, loss = 0.25733519\n",
      "Iteration 75, loss = 0.25345143\n",
      "Iteration 76, loss = 0.24973952\n",
      "Iteration 77, loss = 0.24613535\n",
      "Iteration 78, loss = 0.24260978\n",
      "Iteration 79, loss = 0.23924782\n",
      "Iteration 80, loss = 0.23600812\n",
      "Iteration 81, loss = 0.23282657\n",
      "Iteration 82, loss = 0.22975391\n",
      "Iteration 83, loss = 0.22680010\n",
      "Iteration 84, loss = 0.22389828\n",
      "Iteration 85, loss = 0.22110186\n",
      "Iteration 86, loss = 0.21832982\n",
      "Iteration 87, loss = 0.21564567\n",
      "Iteration 88, loss = 0.21306328\n",
      "Iteration 89, loss = 0.21049013\n",
      "Iteration 90, loss = 0.20799892\n",
      "Iteration 91, loss = 0.20555597\n",
      "Iteration 92, loss = 0.20317085\n",
      "Iteration 93, loss = 0.20080890\n",
      "Iteration 94, loss = 0.19851098\n",
      "Iteration 95, loss = 0.19626577\n",
      "Iteration 96, loss = 0.19402516\n",
      "Iteration 97, loss = 0.19185851\n",
      "Iteration 98, loss = 0.18972069\n",
      "Iteration 99, loss = 0.18762685\n",
      "Iteration 100, loss = 0.18553549\n",
      "CPU times: user 11min 22s, sys: 17.3 s, total: 11min 39s\n",
      "Wall time: 11min 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/volatile/ogrisel/code/scikit-learn/sklearn/neural_network/multilayer_perceptron.py:560: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(algorithm='adam', hidden_layer_sizes=10, max_iter=100, activation='tanh', verbose=100)\n",
    "mlp.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score: 0.813\n",
      "CPU times: user 116 ms, sys: 3.9 ms, total: 120 ms\n",
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"test score: %0.3f\" % mlp.score(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
