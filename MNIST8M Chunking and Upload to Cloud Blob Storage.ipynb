{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Chunking the MNIST8M dataset and store the chunks in the cloud"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from configparser import ConfigParser\n",
      "from time import time\n",
      "\n",
      "import numpy as np\n",
      "from concurrent.futures import ThreadPoolExecutor\n",
      "\n",
      "from libcloud.storage.types import Provider\n",
      "from libcloud.storage.types import ContainerDoesNotExistError\n",
      "from libcloud.storage.types import ObjectDoesNotExistError\n",
      "from libcloud.storage.providers import get_driver"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numpy_folder = os.path.expanduser('~/data/mnist8m/numpy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Uploading the results to a cloud store"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CONFIGFILE_PATH = 'cloudstorage.ini'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use [Apache Libcloud](http://libcloud.apache.org) to upload the chunk objects to a permanent store for later usage in ephemeral VMs. We will store the credential in a configuration file named `cloudstorage.ini`. Here is the expected content for the Windows Azure Cloud:\n",
      "\n",
      "```\n",
      "[account]\n",
      "libcloud_provider = azure_blobs\n",
      "account_name = myacount\n",
      "account_secret = primarykey\n",
      "```\n",
      "\n",
      "On Amazon S3, the config file would look like:\n",
      "\n",
      "```\n",
      "[account]\n",
      "libcloud_provider = s3\n",
      "account_name = aws_key_id\n",
      "account_secret = aws_secret_key\n",
      "```\n",
      "\n",
      "Apache Libcloud supports many more [Cloud Object Store providers](https://ci.apache.org/projects/libcloud/docs/storage/supported_providers.html).\n",
      "\n",
      "The objects will be stored in a specific container. On some providers, the container name must be globally unique (such as is the case for bucket names on S3). On others like Azure, the container names are local to the cloud storage account. In case of conflict, just change the container name: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CONTAINER_NAME = \"mnist8m\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function parse the `cloudstorage.ini` file and build a Libcloud driver instance. This instance is not thread safe, hence we wrap the driver instanciation in a function to be reused in individual threads."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_driver(configfile_path=CONFIGFILE_PATH, section='account'):\n",
      "    config = ConfigParser()\n",
      "    config.read(configfile_path)\n",
      "    provider_name = config.get(section, 'libcloud_provider')\n",
      "    driver_type = get_driver(provider_name)\n",
      "    account_name = config.get(section, 'account_name')\n",
      "    account_secret = config.get(section, 'account_secret')\n",
      "    return driver_type(account_name, account_secret)\n",
      "\n",
      "driver = build_driver()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following utility function checks that a container with a specific name exits on the Cloud Storage provider, otherwise it creates it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_or_create_container(driver, container_name=CONTAINER_NAME):\n",
      "    try:\n",
      "        return driver.get_container(container_name)\n",
      "    except ContainerDoesNotExistError:\n",
      "        return driver.create_container(container_name)\n",
      "    \n",
      "container = get_or_create_container(driver)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now write a function that uploads invidual local files to a target object container. As this function will be called in parallel in various threads we instanciate a dedicated driver inside."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def upload_object(local_folder, object_name, container_name=CONTAINER_NAME, skip_if_exists=True):\n",
      "    driver = build_driver()  # libcloud drivers are not thread-safe\n",
      "    container = get_or_create_container(driver, container_name)\n",
      "    filepath = os.path.join(local_folder, object_name)\n",
      "    if skip_if_exists:\n",
      "        try:\n",
      "            # Check the size to deal with partially uploaded files\n",
      "            ob =  container.get_object(object_name)\n",
      "            if ob.size == os.stat(filepath).st_size:\n",
      "                return ob\n",
      "        except ObjectDoesNotExistError:\n",
      "            pass\n",
      "    return container.upload_object(filepath, object_name,\n",
      "        extra={'content_type': 'application/octet-stream'})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally let us upload all the chunks and labels from the MNIST8M dataset in parallel to speedup the upload. As IPython does not seem to be fully compatible with gevent monkeypatching we will use Python threads to upload data in parallel: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_workers = 10\n",
      "filenames = os.listdir(numpy_folder)\n",
      "\n",
      "tic = time()\n",
      "with ThreadPoolExecutor(max_workers=n_workers) as e:\n",
      "    for f in filenames:\n",
      "        e.submit(upload_object, local_folder, f)\n",
      "print(\"Uploaded {} files with {} workers in {:0.3f}s\".format(\n",
      "      len(filenames), n_workers, time() - tic))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Uploaded 83 files with 10 workers in 281.750s\n"
       ]
      }
     ],
     "prompt_number": 106
    }
   ],
   "metadata": {}
  }
 ]
}